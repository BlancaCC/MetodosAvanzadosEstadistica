# Ejercicio 9 

 Genera aleatoriamente una variable regresora 
$X$ y un vector aleatorio $\epsilon$ de longitud 
$n = 100$, con distribución normal estándar e independientes.

Genera la variable respuesta de acuerdo con el modelo:
$$
Y = X + X^2 + X^3 + \epsilon. 
$$
```{r}
library(purrr)
library(ggplot2)
library(ramify)
```

```{r}
set.seed(1)
n <- 100 
X_distribution <- rnorm 
error_distribution <- rnorm
modelo_1 <- function (x) {
  return (x + x^2 + x^3)
}

Y_distribution <- function (n, t){
  x <- X_distribution(n)
  e <- error_distribution(n)
  return (sapply(x, t) + e)
}


Y_1 <- Y_distribution(n, modelo_1)
```

```{r}
X <- X_distribution(n)
# Generamos las potencias
datos <- data.frame('X' = X)
#datos[1] <- X
for (i in 2:10) {
  datos[i] <- datos[1]*datos[i-1]
}

# Modelo1 
Y <- unlist(as.list(datos[1]+datos[2] + datos[3] + error_distribution(n)))
datos$'Y' <- Y
datos2 <- datos
nombre_variables <- c('X1', 'X2',  'X3', 'X4', 'X5',
                     'X6',  'X7', 'X8', 'X9', 'X10', 'y')
colnames(datos) <- nombre_variables
colnames(datos2) <- nombre_variables
datos2$'Y_ideal' <- sapply(X, function(x)(x*(1 + x*(1 + x))))

p <- ggplot(datos2) + 
  geom_point( aes(X, Y), colour = 'black' ) +
  geom_line(aes(X, Y_ideal), colour = 'red' ) +
  ggtitle('Muestra ruidosa y función ideal')
  
p
```

##  Selecciona el modelo óptimo 

Entre todos los submodelos que contienen como variables regresoras $X, X^2, X^3, \ldots, X^{10}$. 
¿Cuál es el mejor modelo de acuerdo con los criterios $C_p$, BIC y $R_a^2$?

**Solución** 

Para poder realizar estas comparaciones utilizaremos la función [`leaps::regsubsets`](https://www.rdocumentation.org/packages/leaps/versions/3.1/topics/regsubsets). 

Donde los argumentos que nos interesan son: 

- `datos`  
- `method` donde indicaremos de si se trata de `exhaustive` o `forward`.  
- `nvmax` número máximo de subconjuntos a examinar, la pondremos al máximo de parámetros que tenemos.
- `intercept` Si añadimos sesgo o no. 
- `adjr2` para Adjusted r-squared. 
- `cp` para Mallows's CP. 
- `bic` para Schwartz's information criterion BIC. 

Nodemos además que en el modelos que nos piden no hay término independiente 
(ya que por el contrario habría añadido un `1` a las variables regresoras ),
luego tendremos que hacer  `intercept = FALSE`. 

```{R}
subconjuntos_a_examinar <- 10
modelo_exhaustivo <- leaps::regsubsets(
  y ~ ., 
  data=datos,
  nvmax= subconjuntos_a_examinar, # para que utilice todos las componentes posibles,
  intercept = FALSE
  )

resumen_1 <- summary(modelo_exhaustivo)
resumen_1
```

```{r}
# Función para pintar 
pinta_ajuste <- function(y_criterio, label){
  plot_data <- data.frame(
    x = 1:subconjuntos_a_examinar,
    y = y_criterio
  )
  print(plot_data)
  y_label <- label
  
  plot(plot_data$x, plot_data$y, 
       type = "l",
       xlab = 'Número de variables',
       ylab = y_label)
}

```

### Mejor modelo de acorde a R cuadrado ajustado 

```{r}
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
```
Será mejor donde alcance un máximo, esto es en $p = 4$ es decir utilizando 4 parámetros. 
Vamos a proceder a analizar los coeficientes. 

```{r}
# Mejores elementos
#Selection Algorithm: exhaustive
#         X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "

modelo_r2_a <- lm(y ~ X1 + X2 + X3  + X5 -1, data=datos)
summary(modelo_r2_a)

y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)

datos2$'Y_r2a' <- y_predit_r2_a 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_r2a), colour = 'green' )
  
p

```

Como vemos más variables lo que hacen es acercarse al ruido. 

### Cp
```{r}
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")

```

Vemos que con este criterio alcanza un mínimo en tres variables. 
es decir que el mejor de los resultados sería 

```
Selection Algorithm: exhaustive
         X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
3  ( 1 )  "*" "*" "*" " " " " " " " " " " " " " "
```
que si entrenamos obtendríamos

```{r}
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)

y_predit_cp <- predict(object=modelo_cp, newdata=datos)

datos2$'Y_cp' <- y_predit_cp 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_cp), colour = 'green' )
  
p
```
### BIC 



```{r}
pinta_ajuste(resumen_1$bic, "Bic")

cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
```
```{r}
modelo_bic<- lm(y ~ X1 + X2 + X3 -1, data=datos)
summary(modelo_bic)

y_predit_bic <- predict(object=modelo_bic, newdata=datos)

datos2$'Y_bic' <- y_predit_bic 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_bic), colour = 'green' )
  
p
```
## Usando ahora el método iterativo  

```{R}

modelo_iterativo_delante <- leaps::regsubsets(
  y ~ ., 
  data=datos, 
  method = "forward",
  nvmax = subconjuntos_a_examinar,
  intercept = FALSE
  )
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
```
Observaciones: 

Como vemos ahora las variables seleccionadas para $p$ parámetros están incluidas 
en el modelo con $p+1$ parámetros. Esto es lo esperado ya que es el comportamiento de forward 
con el fin de mejorar el rendimiento. 


### Mejor modelo de acorde a R cuadrado ajustado  con modelo iterativo hacia delante

```{r}
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
```
Será mejor donde alcance un máximo, esto es en $p = 8$ es decir utilizando 8 parámetros. 
Vamos a proceder a analizar los coeficientes. 

```{r}
#          X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "


modelo_r2_a <- lm(y ~ X1 + X2 + X3 +  X5 +-1, data=datos)
summary(modelo_r2_a)

y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)

datos2$'Y_r2a' <- y_predit_r2_a 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_r2a), colour = 'green' )

p
```

Como vemos más variables lo que hacen es acercarse al ruido. 
El error `djusted R-squared:  0.968` coincide con el del criterio. 

### Cp
```{r}
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
summary(modelo_iterativo_delante)
```

Vemos que con este criterio alcanza un mínimo en tres variables. 


```{r}
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)

y_predit_cp <- predict(object=modelo_cp, newdata=datos)

datos2$'Y_cp' <- y_predit_cp 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_cp), colour = 'green' )
  
p
```
### BIC 



```{r}
pinta_ajuste(resumen_2$bic, "Bic")

cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
```
```{r}
modelo_bic<- lm(y ~ X1 + X2 +  X3  -1, data=datos)
summary(modelo_bic)

y_predit_bic <- predict(object=modelo_bic, newdata=datos)

datos2$'Y_bic' <- y_predit_bic 

p <- ggplot(datos2) + 
  geom_point( aes(X1, Y), colour = 'black' ) +
  geom_line(aes(X1, Y_ideal), colour = 'red' ) +
  geom_line(aes(X1, Y_bic), colour = 'green' )
  
p
print('Los coefficientes del modelo son')
coef(modelo_bic)
```

## Lasso 
```{r}
library(glmnet)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
```

La ventaja que tiene lasso es que permite eliminar ciertos coeficientes esto queda de manifiesto en que la mayoría son nulos. 


## Genera ahora las respuestas a partir del modelo 

$$
Y = X^7 + \epsilon.
$$

### Generación de los datos 
```{r}
# Generamos las potencias
datos <- data.frame('1'=X)
for (i in 2:10) {
  datos[i] <- datos[1]*datos[i-1]
}
# Modelo1 
Y <- unlist(
  as.list(
    datos[7] + error_distribution(n)
  )
)
datos$'y' <- Y
colnames(datos) <- nombre_variables
```


### Modelo óptimo dentro de submodelos  
 
```{R}
modelo_exhaustivo <- leaps::regsubsets(
  y ~ ., 
  data=datos,
  nvmax = subconjuntos_a_examinar,
  intercept = FALSE
  )
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
```

Vemos que ahora  a pesar de estar en un método exhaustivo, $X^7$ (el que sabemos que forma parte del modelo ideal)
se encuentra en todos. 
Esto puede entenderse como que al ser solo una *es más simple que la encuentre* y que no se compense con nada. 

### Submodelos óptimos
```{r}
print("Submodelos óptimos")

pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")

pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")

pinta_ajuste(resumen_1$bic, "Bic")

cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")

summary(modelo_exhaustivo)
```

### Submodelos utilizando un método iterativo 

```{R}

modelo_iterativo_delante <- leaps::regsubsets(
  y ~ ., 
  data=datos, 
  method = "forward",
  nvmax = subconjuntos_a_examinar,
  intercept = FALSE
  )
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
```

```{R}
print("Submodelos utilizando el método iterativo \n")
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")

pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")

pinta_ajuste(resumen_2$bic, "Bic")

cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
```

## Lasso 
```{r}
x <- datos[c(1:10)]

y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
```

### Conclusiones  

A la vista de los resultados el modelo BIC es el más robusto a la hora de determinar el número de parámetros correcto. 
Lasso es una alternativa razonable para reducir el número de variables y minimizar el error. 

