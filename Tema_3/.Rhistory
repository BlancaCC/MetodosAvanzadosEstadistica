summary(modelo_exhaustivo)
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_bic<- lm(y ~ X1 + X1.2 + X1.3 -1, data=datos)
modelo_bic<- lm(y ~ X1 + X2 + X3 -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
modelo_iterativo_delante <- leaps::regsubsets(
y ~ .,
data=datos,
method = "forward",
nvmax = subconjuntos_a_examinar,
intercept = FALSE
)
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
#          X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "
modelo_r2_a <- lm(y ~ X1 + X2 + X3 +  X5 +-1, data=datos)
summary(modelo_r2_a)
y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)
datos2$'Y_r2a' <- y_predit_r2_a
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_r2a), colour = 'green' )
p
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
summary(modelo_iterativo_delante)
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_2$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
modelo_bic<- lm(y ~ X1 + X2 +  X3  -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
print('Los coefficientes del modelo son')
coef(modelo_bic)
# Generamos las potencias
datos <- data.frame('1'=X)
for (i in 2:10) {
datos[i] <- datos[1]*datos[i-1]
}
# Modelo1
Y <- unlist(
as.list(
datos[7] + error_distribution(n)
)
)
datos$'y' <- Y
colnames(datos) <- nombre_variables
print("Submodelos óptimos")
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_exhaustivo <- leaps::regsubsets(
y ~ .,
data=datos,
nvmax = subconjuntos_a_examinar
)
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
library(glmnet)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
library(purrr)
library(ggplot2)
library(ramify)
set.seed(1)
n <- 100
X_distribution <- rnorm
error_distribution <- rnorm
modelo_1 <- function (x) {
return (x + x^2 + x^3)
}
Y_distribution <- function (n, t){
x <- X_distribution(n)
e <- error_distribution(n)
return (sapply(x, t) + e)
}
Y_1 <- Y_distribution(n, modelo_1)
X <- X_distribution(n)
# Generamos las potencias
datos <- data.frame('X' = X)
#datos[1] <- X
for (i in 2:10) {
datos[i] <- datos[1]*datos[i-1]
}
# Modelo1
Y <- unlist(as.list(datos[1]+datos[2] + datos[3] + error_distribution(n)))
datos$'Y' <- Y
datos2 <- datos
nombre_variables <- c('X1', 'X2',  'X3', 'X4', 'X5',
'X6',  'X7', 'X8', 'X9', 'X10', 'y')
colnames(datos) <- nombre_variables
colnames(datos2) <- nombre_variables
datos2$'Y_ideal' <- sapply(X, function(x)(x*(1 + x*(1 + x))))
p <- ggplot(datos2) +
geom_point( aes(X, Y), colour = 'black' ) +
geom_line(aes(X, Y_ideal), colour = 'red' ) +
ggtitle('Muestra ruidosa y función ideal')
p
subconjuntos_a_examinar <- 10
modelo_exhaustivo <- leaps::regsubsets(
y ~ .,
data=datos,
nvmax= subconjuntos_a_examinar, # para que utilice todos las componentes posibles,
intercept = FALSE
)
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
# Función para pintar
pinta_ajuste <- function(y_criterio, label){
plot_data <- data.frame(
x = 1:subconjuntos_a_examinar,
y = y_criterio
)
print(plot_data)
y_label <- label
plot(plot_data$x, plot_data$y,
type = "l",
xlab = 'Número de variables',
ylab = y_label)
}
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
# Mejores elementos
#Selection Algorithm: exhaustive
#         X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "
modelo_r2_a <- lm(y ~ X1 + X2 + X3  + X5 -1, data=datos)
summary(modelo_r2_a)
y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)
datos2$'Y_r2a' <- y_predit_r2_a
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_r2a), colour = 'green' )
p
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_bic<- lm(y ~ X1 + X2 + X3 -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
modelo_iterativo_delante <- leaps::regsubsets(
y ~ .,
data=datos,
method = "forward",
nvmax = subconjuntos_a_examinar,
intercept = FALSE
)
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
#          X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "
modelo_r2_a <- lm(y ~ X1 + X2 + X3 +  X5 +-1, data=datos)
summary(modelo_r2_a)
y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)
datos2$'Y_r2a' <- y_predit_r2_a
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_r2a), colour = 'green' )
p
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
summary(modelo_iterativo_delante)
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_2$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
modelo_bic<- lm(y ~ X1 + X2 +  X3  -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
print('Los coefficientes del modelo son')
coef(modelo_bic)
library(glmnet)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
# Generamos las potencias
datos <- data.frame('1'=X)
for (i in 2:10) {
datos[i] <- datos[1]*datos[i-1]
}
# Modelo1
Y <- unlist(
as.list(
datos[7] + error_distribution(n)
)
)
datos$'y' <- Y
colnames(datos) <- nombre_variables
modelo_exhaustivo <- leaps::regsubsets(
y ~ .,
data=datos,
nvmax = subconjuntos_a_examinar
)
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
print("Submodelos óptimos")
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_iterativo_delante <- leaps::regsubsets(
y ~ .,
data=datos,
method = "forward",
nvmax = subconjuntos_a_examinar
)
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
print("Submodelos utilizando el método iterativo \n")
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
pinta_ajuste(resumen_2$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
library(purrr)
library(ggplot2)
library(ramify)
set.seed(1)
n <- 100
X_distribution <- rnorm
error_distribution <- rnorm
modelo_1 <- function (x) {
return (x + x^2 + x^3)
}
Y_distribution <- function (n, t){
x <- X_distribution(n)
e <- error_distribution(n)
return (sapply(x, t) + e)
}
Y_1 <- Y_distribution(n, modelo_1)
X <- X_distribution(n)
# Generamos las potencias
datos <- data.frame('X' = X)
#datos[1] <- X
for (i in 2:10) {
datos[i] <- datos[1]*datos[i-1]
}
# Modelo1
Y <- unlist(as.list(datos[1]+datos[2] + datos[3] + error_distribution(n)))
datos$'Y' <- Y
datos2 <- datos
nombre_variables <- c('X1', 'X2',  'X3', 'X4', 'X5',
'X6',  'X7', 'X8', 'X9', 'X10', 'y')
colnames(datos) <- nombre_variables
colnames(datos2) <- nombre_variables
datos2$'Y_ideal' <- sapply(X, function(x)(x*(1 + x*(1 + x))))
p <- ggplot(datos2) +
geom_point( aes(X, Y), colour = 'black' ) +
geom_line(aes(X, Y_ideal), colour = 'red' ) +
ggtitle('Muestra ruidosa y función ideal')
p
subconjuntos_a_examinar <- 10
modelo_exhaustivo <- leaps::regsubsets(
y ~ .,
data=datos,
nvmax= subconjuntos_a_examinar, # para que utilice todos las componentes posibles,
intercept = FALSE
)
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
# Función para pintar
pinta_ajuste <- function(y_criterio, label){
plot_data <- data.frame(
x = 1:subconjuntos_a_examinar,
y = y_criterio
)
print(plot_data)
y_label <- label
plot(plot_data$x, plot_data$y,
type = "l",
xlab = 'Número de variables',
ylab = y_label)
}
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
# Mejores elementos
#Selection Algorithm: exhaustive
#         X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "
modelo_r2_a <- lm(y ~ X1 + X2 + X3  + X5 -1, data=datos)
summary(modelo_r2_a)
y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)
datos2$'Y_r2a' <- y_predit_r2_a
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_r2a), colour = 'green' )
p
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_bic<- lm(y ~ X1 + X2 + X3 -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
modelo_iterativo_delante <- leaps::regsubsets(
y ~ .,
data=datos,
method = "forward",
nvmax = subconjuntos_a_examinar,
intercept = FALSE
)
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
#          X1  X2  X3  X4  X5  X6  X7  X8  X9  X10
#4  ( 1 )  "*" "*" "*" " " "*" " " " " " " " " " "
modelo_r2_a <- lm(y ~ X1 + X2 + X3 +  X5 +-1, data=datos)
summary(modelo_r2_a)
y_predit_r2_a <- predict(object=modelo_r2_a, newdata=datos)
datos2$'Y_r2a' <- y_predit_r2_a
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_r2a), colour = 'green' )
p
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
summary(modelo_iterativo_delante)
modelo_cp<- lm(y ~ X1 + X2 + X3  -1, data=datos)
summary(modelo_cp)
y_predit_cp <- predict(object=modelo_cp, newdata=datos)
datos2$'Y_cp' <- y_predit_cp
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_cp), colour = 'green' )
p
pinta_ajuste(resumen_2$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
modelo_bic<- lm(y ~ X1 + X2 +  X3  -1, data=datos)
summary(modelo_bic)
y_predit_bic <- predict(object=modelo_bic, newdata=datos)
datos2$'Y_bic' <- y_predit_bic
p <- ggplot(datos2) +
geom_point( aes(X1, Y), colour = 'black' ) +
geom_line(aes(X1, Y_ideal), colour = 'red' ) +
geom_line(aes(X1, Y_bic), colour = 'green' )
p
print('Los coefficientes del modelo son')
coef(modelo_bic)
library(glmnet)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
# Generamos las potencias
datos <- data.frame('1'=X)
for (i in 2:10) {
datos[i] <- datos[1]*datos[i-1]
}
# Modelo1
Y <- unlist(
as.list(
datos[7] + error_distribution(n)
)
)
datos$'y' <- Y
colnames(datos) <- nombre_variables
modelo_exhaustivo <- leaps::regsubsets(
y ~ .,
data=datos,
nvmax = subconjuntos_a_examinar
)
resumen_1 <- summary(modelo_exhaustivo)
resumen_1
print("Submodelos óptimos")
pinta_ajuste(resumen_1$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_1$adjr2), "parametros")
pinta_ajuste(resumen_1$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_1$cp), "parametros\n")
pinta_ajuste(resumen_1$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_exhaustivo)
modelo_iterativo_delante <- leaps::regsubsets(
y ~ .,
data=datos,
method = "forward",
nvmax = subconjuntos_a_examinar
)
resumen_2 <- summary(modelo_iterativo_delante)
resumen_2
print("Submodelos utilizando el método iterativo \n")
pinta_ajuste(resumen_2$adjr2, "R2 ajustado")
cat("Se alcanza el máximo con ", which.max(resumen_2$adjr2), "parametros")
pinta_ajuste(resumen_2$cp, "Cp")
cat("Con el criterio CP, el mejor modelo tiene ", which.min(resumen_2$cp), "parametros\n")
pinta_ajuste(resumen_2$bic, "Bic")
cat("Con el criterio BIC, el mejor modelo tiene ", which.min(resumen_1$bic), "parametros\n")
summary(modelo_iterativo_delante)
x <- datos[c(1:10)]
y <- datos$y
modelo_lasso <- glmnet(x, y, alpha = 1, intercept=FALSE)   # alpha = 1 (lasso); alpha = 0 (ridge)
plot(modelo_lasso, xvar='lambda', label=TRUE)
coef(modelo_lasso, s = 0.1)
#help(coef)
